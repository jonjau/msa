{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spam-detector\n",
    "\n",
    "%2020-04-19\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading .data file from local copy of database...\n",
      "Reading .names file from local copy of database...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "import requests\n",
    "import os\n",
    "\n",
    "DATABASE_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/'\n",
    "LOCAL_DATABASE_PATH = 'spambase'\n",
    "\n",
    "# Try opening local copies first before fetching from online database\n",
    "try:\n",
    "    df = pd.read_csv(os.path.join('spambase-data', 'spambase.data'),\n",
    "                      header=None, index_col=False)\n",
    "    print('Reading .data file from local copy of database...')\n",
    "except OSError:\n",
    "    df = pd.read_csv(urllib.parse.urljoin(database_url, 'spambase.data'),\n",
    "                     header=None, index_col=False)\n",
    "    print('Reading .data file from online of database...')\n",
    "\n",
    "try:\n",
    "    with open(os.path.join('spambase-data', 'spambase.names')) as f:\n",
    "        names_file_text = f.read()\n",
    "        print('Reading .names file from local copy of database...')\n",
    "except OSError:\n",
    "    names_file_text = requests.get(urllib.parse.urljoin(database_url, 'spambase.names')).text\n",
    "    print('Reading .names file from online database...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attributes are specified in the .names format: http://www.cs.washington.edu/dm/vfml/appendixes/c45.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attribute_names(names_file_text):\n",
    "    # Anything between a '|' and the end of the line is ignored\n",
    "    strip_comments = lambda line : line.split('|',1)[0]\n",
    "    attr_names = []\n",
    "    read_classes = False\n",
    "    for line in names_file_text.splitlines():\n",
    "        if len(line.strip()) == 0 or line[0] == '|':\n",
    "            continue\n",
    "        elif not read_classes:\n",
    "            classes = strip_comments(line).split(',')\n",
    "            read_classes = True\n",
    "        else:\n",
    "            attr_name, attr_type = strip_comments(line).split(':')\n",
    "            attr_names.append(attr_name)\n",
    "    return attr_names\n",
    "\n",
    "# Add feature name to last column\n",
    "df.columns = get_attribute_names(names_file_text) + ['spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Instances: 4601 (1813 Spam = 39.4%)\n",
    "# Check for null entries: none found\n",
    "#df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_capital_run_length_data(df):\n",
    "    crl = df.filter(regex=('capital_run_length*'))\n",
    "    # Min-Max normalisation\n",
    "    normalise = lambda col : (col-col.min())/(col.max()-col.min())\n",
    "    crl = (crl-crl.min())/(crl.max()-crl.min())\n",
    "    for col_name in crl.columns:\n",
    "        df[col_name] = normalise(df[col_name])\n",
    "    return df\n",
    "    \n",
    "\n",
    "features = df.drop('spam', axis=1)\n",
    "features = normalise_capital_run_length_data(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_SEED = 9\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(features, df['spam'], test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model1 = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "model1 = model1.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model2 = LogisticRegression()\n",
    "model2 = model2.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy       : 0.9587404994571118\n",
      "ROC AUC        : 0.9876847658388493\n",
      "TP, FP, FN, TN : [552, 14, 24, 331]\n",
      "Precision      : 0.9594202898550724\n",
      "Recall         : 0.9323943661971831\n",
      "\n",
      "Accuracy       : 0.9370249728555917\n",
      "ROC AUC        : 0.9778679141989748\n",
      "TP, FP, FN, TN : [542, 24, 34, 321]\n",
      "Precision      : 0.9304347826086956\n",
      "Recall         : 0.9042253521126761\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as skm\n",
    "def display_metrics(model, inputs, outputs):\n",
    "    score = model.score(inputs, outputs)\n",
    "    probabilities = model.predict_proba(inputs)\n",
    "    predictions = model.predict(inputs)\n",
    "    \n",
    "    roc_auc = skm.roc_auc_score(outputs, probabilities[:, 1])\n",
    "    tp, fp, fn, tn = skm.confusion_matrix(outputs, predictions).ravel()\n",
    "    precision = skm.precision_score(outputs, predictions)\n",
    "    recall = skm.recall_score(outputs, predictions)\n",
    "    \n",
    "    metrics_report = (\n",
    "        f'Accuracy       : {score}\\n'\n",
    "        f'ROC AUC        : {roc_auc}\\n'\n",
    "        f'TP, FP, FN, TN : {[tp, fp, fn, tn]}\\n'\n",
    "        f'Precision      : {precision}\\n'\n",
    "        f'Recall         : {recall}\\n'\n",
    "    )\n",
    "    print(metrics_report)\n",
    "\n",
    "display_metrics(model1, test_x, test_y)\n",
    "#display_metrics(model1, train_x, train_y)\n",
    "display_metrics(model2, test_x, test_y)\n",
    "#display_metrics(model2, train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
